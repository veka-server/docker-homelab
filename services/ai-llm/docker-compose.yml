services:
  mcpo:
    image: ghcr.io/open-webui/mcpo:main
    container_name: mcpo-server
    volumes:
      - ${ROOT_PATH}/mcpo-config.json:/app/config.json:ro
    command: >
      --port 8000
      --config /app/config.json
    restart: unless-stopped
    networks:
      - custom-network
      - mcp-network
    labels:
      - "homepage.group=AI & LLM"
      - "homepage.name=MCPO"
      - "homepage.icon=/icons/mcp.png"
      - "homepage.description=MCP Protocol Server"

  open-webui-standalone:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-standalone
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - ENABLE_OPENAI_API=0
      - HF_HUB_OFFLINE=1
      - OFFLINE_MODE=1
    volumes:
      - ${ROOT_PATH}/open-webui:/app/backend/data
#    ports:
#      - "8080:8080"
    networks:
      - no-internet
      - mcp-network
    restart: unless-stopped
    labels:
      - "homepage.group=AI & LLM"
      - "homepage.name=Open WebUI"
      - "homepage.icon=open-webui-light.png"
      - "homepage.href=https://open-webui.loc.ovh"
      - "homepage.description=Interface Web pour les mod√®les LLM"
      - "homepage.weight=1"
      - "traefik.enable=true"
      - "traefik.http.routers.open-webui.rule=Host(`open-webui.loc.ovh`)"
      - "traefik.http.routers.open-webui.entrypoints=web,websecure"
      - "traefik.http.routers.open-webui.tls=true"
      - "traefik.http.services.open-webui.loadbalancer.server.port=8080"

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_KEEP_ALIVE=0
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    networks:
      - no-internet
      - custom-network
    restart: unless-stopped
    labels:
      - "homepage.group=AI & LLM"
      - "homepage.name=Ollama"
      - "homepage.icon=ollama-dark.png"
      - "homepage.description=Moteur LLM local"
      - "homepage.weight=2"

volumes:
  ollama:
