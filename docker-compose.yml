networks:
  no-internet:
    internal: true
  custom-network:
    driver: bridge

volumes:
  comfyui-inputs:
  ollama:
  open-webui:
  portainer_data:

services:

  downloader:
    image: alpine
    volumes:
      - /home/veka/models:/models
    command: >
      sh -c "apk add --no-cache wget git git-lfs coreutils \
      && [ -f /models/unet/flux1-dev-Q8_0.gguf ] ||  wget -nv -O /models/unet/flux1-dev-Q8_0.gguf https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q8_0.gguf?download=true \
      && [ -f /models/unet/flux1-schnell-Q8_0.gguf ] ||  wget -nv -O /models/unet/flux1-schnell-Q8_0.gguf https://huggingface.co/city96/FLUX.1-schnell-gguf/resolve/main/flux1-schnell-Q8_0.gguf?download=true \
      && [ -f /models/unet/hidream-i1-full-Q8_0.gguf ] ||  wget -nv -O /models/unet/hidream-i1-full-Q8_0.gguf https://huggingface.co/city96/HiDream-I1-Full-gguf/resolve/main/hidream-i1-full-Q8_0.gguf?download=true \
      && [ -f /models/clip/flux_clip_l.safetensors ] ||  wget -nv -O /models/clip/flux_clip_l.safetensors https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true \
      && [ -f /models/clip/t5-v1_1-xxl-encoder-Q8_0.gguf ] ||  wget -nv -O /models/clip/t5-v1_1-xxl-encoder-Q8_0.gguf https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf/resolve/main/t5-v1_1-xxl-encoder-Q8_0.gguf?download=true \
      && [ -f /models/vae/flux_vae.safetensors ] ||  wget -nv -O /models/vae/flux_vae.safetensors https://huggingface.co/foxmail/flux_vae/resolve/main/ae.safetensors?download=true \
      && [ -f /models/vae/hunyuan_video_vae_bf16.safetensors ] ||  wget -nv -O /models/vae/hunyuan_video_vae_bf16.safetensors https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true \
      && [ -f /models/vae/hidream_ae.safetensors ] ||  wget -nv -O /models/vae/hidream_ae.safetensors https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/vae/ae.safetensors?download=true \
      && [ -f /models/diffusion_models/hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors ] ||  wget -nv -O /models/diffusion_models/hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_720_cfgdistill_fp8_e4m3fn.safetensors?download=true \
      && [ -f /models/text_encoders/hunyuan_video_clip_l.safetensors ] ||  wget -nv -O /models/text_encoders/hunyuan_video_clip_l.safetensors https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true \
      && [ -f /models/text_encoders/llava_llama3_fp8_scaled.safetensors ] ||  wget -nv -O /models/text_encoders/llava_llama3_fp8_scaled.safetensors https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true \
      && [ -f /models/text_encoders/clip_l_hidream.safetensors ] ||  wget -nv -O /models/text_encoders/clip_l_hidream.safetensors https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/text_encoders/clip_l_hidream.safetensors?download=true \
      && [ -f /models/text_encoders/clip_g_hidream.safetensors ] ||  wget -nv -O /models/text_encoders/clip_g_hidream.safetensors https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/text_encoders/clip_g_hidream.safetensors?download=true \
      && [ -f /models/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors ] ||  wget -nv -O /models/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors?download=true \
      && [ -f /models/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors ] ||  wget -nv -O /models/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors?download=true \
      && [ -f /models/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors ] ||  wget -nv -O /models/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true \
      && [ -f /models/clip_vision/llava_llama3_vision.safetensors ] ||  wget -nv -O /models/clip_vision/llava_llama3_vision.safetensors https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true \
      && [ -f /models/unet/hunyuan_video_I2V-Q8_0.gguf ] ||  wget -nv -O /models/unet/hunyuan_video_I2V-Q8_0.gguf https://huggingface.co/Kijai/HunyuanVideo_comfy/resolve/main/hunyuan_video_I2V-Q8_0.gguf?download=true \
      && if [ -d "/models/clip/siglip-so400m-patch14-384/.git" ]; then git -C /models/clip/siglip-so400m-patch14-384 pull; else git clone https://huggingface.co/google/siglip-so400m-patch14-384 /clip/siglip-so400m-patch14-384; fi \
      && if [ -d "/models/LLM/Meta-Llama-3.1-8B-Instruct-bnb-4bit/.git" ]; then git -C /models/LLM/Meta-Llama-3.1-8B-Instruct-bnb-4bit pull; else git clone https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit /models/LLM/Meta-Llama-3.1-8B-Instruct-bnb-4bit ; fi \
      && if [ -d "/models/Joy_caption_two/.git" ]; then git -C /models/Joy_caption_two pull; else git clone  https://huggingface.co/spaces/fancyfeast/joy-caption-alpha-two /models/Joy_caption_two ; fi \
      && [ -f /models/checkpoints/ace_step_v1_3.5b.safetensors ] ||  wget -nv -O /models/checkpoints/ace_step_v1_3.5b.safetensors https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/resolve/main/all_in_one/ace_step_v1_3.5b.safetensors?download=true \
      && [ -f /models/unet/Wan2.1-VACE-14B-Q4_K_S.gguf ] ||  wget -nv -O /models/unet/Wan2.1-VACE-14B-Q4_K_S.gguf https://huggingface.co/Aitrepreneur/FLX/resolve/main/Wan2.1-VACE-14B-Q4_K_S.gguf?download=true \
      && [ -f /models/loras/Wan21_CausVid_14B_T2V_lora_rank32.safetensors ] ||  wget -nv -O /models/loras/Wan21_CausVid_14B_T2V_lora_rank32.safetensors https://huggingface.co/Aitrepreneur/FLX/resolve/main/Wan21_CausVid_14B_T2V_lora_rank32.safetensors?download=true \
      && [ -f /models/vae/wan_2.1_vae.safetensors ] ||  wget -nv -O /models/vae/wan_2.1_vae.safetensors https://huggingface.co/Aitrepreneur/FLX/resolve/main/wan_2.1_vae.safetensors?download=true \
      && [ -f /models/unet/chroma.gguf ] || wget -nv -O /models/unet/chroma.gguf https://huggingface.co/silveroxides/Chroma-GGUF/resolve/main/chroma-unlocked-v31/chroma-unlocked-v31-Q5_0.gguf \
      && [ -f /models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors ] || wget -nv -O /models/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors https://huggingface.co/Aitrepreneur/FLX/resolve/main/umt5_xxl_fp8_e4m3fn_scaled.safetensors \
      && [ -f /models/checkpoints/ema-fp8.safetensors ] || wget -nv -O /models/checkpoints/ema-fp8.safetensors https://huggingface.co/boricuapab/Bagel-7B-MoT-fp8/resolve/main/ema-fp8.safetensors \
      && [ -f /models/checkpoints/CyberIllustrious_V4.safetensors ] || wget -nv -O /models/checkpoints/CyberIllustrious_V4.safetensors https://huggingface.co/cyberdelia/CyberIllustrious/resolve/main/CyberIllustrious_V4.safetensors \
      && [ -f /models/unet/flux1-kontext-dev-Q6_K.gguf ] || wget -nv -O /models/unet/flux1-kontext-dev-Q6_K.gguf https://huggingface.co/bullerwins/FLUX.1-Kontext-dev-GGUF/resolve/main/flux1-kontext-dev-Q6_K.gguf \      
      "
  dns:
    build:
      context: https://github.com/veka-server/docker-dns.git#main
    container_name: dns
    ports:
      - "53:53/udp"
      - "53:53/tcp"
    networks:
      - custom-network
    restart: unless-stopped

  florence-promptgen:
    build:
      context: https://github.com/veka-server/docker-Florence-2-large-PromptGen-v2.0.git#main
    container_name: florence-promptgen
    ports:
      - "5000:5000"
    networks:
      - custom-network
    restart: unless-stopped

  comfyui:
    build:
      context: https://github.com/veka-server/docker-comfyui.git#main
    container_name: comfyui
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - VNC_PASSWORD=5555
    ports:
      - "8188:8188"
    volumes:
      - /home/veka/models/checkpoints:/app/comfyui/models/checkpoints
      - /home/veka/models/clip:/app/comfyui/models/clip
      - /home/veka/models/clip_vision:/app/comfyui/models/clip_vision
      - /home/veka/models/unet:/app/comfyui/models/unet
      - /home/veka/models/vae:/app/comfyui/models/vae
      - /home/veka/models/loras:/app/comfyui/models/loras
      - /home/veka/models/upscale_models:/app/comfyui/models/upscale_models
      - /home/veka/models/controlnet:/app/comfyui/models/controlnet
      - /home/veka/models/output:/app/comfyui/output
      - /home/veka/models/LLM:/app/comfyui/models/LLM
      - /home/veka/models/diffusion_models:/app/comfyui/models/diffusion_models
      - /home/veka/models/text_encoders:/app/comfyui/models/text_encoders
      - /home/veka/models/Joy_caption_two/cgrkzexw-599808:/app/comfyui/models/Joy_caption_two
      - comfyui-inputs:/app/comfyui/input:rw
      - /home/veka/models/workflow:/app/comfyui/user/default/workflows
    networks:
      - no-internet
    restart: unless-stopped
    depends_on:
      - downloader

  viewcomfy:
    build:
      context: https://github.com/veka-server/ViewComfy.git#main
    container_name: viewcomfy
    environment:
      - COMFYUI_BASE_URL=comfyui
      - COMFYUI_PORT=8188
      - NEXT_PUBLIC_VIEW_MODE=false
    ports:
      - "3000:3000"
    volumes:
      - /home/veka/view_comfy.json:/app/view_comfy.json
    networks:
      - no-internet
    restart: unless-stopped

  ollama:
    image: ollama/ollama
    container_name: ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - OLLAMA_KEEP_ALIVE=0
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    networks:
      - no-internet
      - custom-network
    restart: unless-stopped

  open-webui-standalone:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-standalone
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - ENABLE_OPENAI_API=0
      - HF_HUB_OFFLINE=1
      - OFFLINE_MODE=1
      - AUDIO_TTS_ENGINE=openai
      - AUDIO_TTS_MODEL=tts-1-hd
      - AUDIO_TTS_OPENAI_API_BASE_URL=http://openedai-speech:8000/v1
      - AUDIO_TTS_OPENAI_API_KEY=sk-111111111
      - AUDIO_TTS_VOICE=onyx
    volumes:
      - open-webui:/app/backend/data
    networks:
      - no-internet
    restart: unless-stopped

  explorer:
    image: hurlenko/filebrowser
    container_name: explorer
    user: "${UID}:${GID}"
    environment:
      - FB_BASEURL=/filebrowser
    ports:
      - "8080:8080"
    volumes:
      - /home/veka:/data/veka
      - /CONFIG_DIR:/config
    networks:
      - no-internet
    restart: unless-stopped

  nginx-reverse-proxy:
    build:
      context: https://github.com/veka-server/docker-nginx-reverse-proxy.git#main
    container_name: nginx-reverse-proxy
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /home/veka/nginx-ssl:/etc/nginx/ssl
    networks:
      - no-internet
      - custom-network
    restart: unless-stopped

  openedai-speech:
    image: ghcr.io/matatonic/openedai-speech
    container_name: openedai-speech
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    environment:
      - EXTRA_ARGS=--unload-timer=10
    volumes:
      - /home/veka/openedai/voices:/app/voices
      - /home/veka/openedai/config:/app/config
    networks:
      - no-internet
    restart: unless-stopped

  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    ports:
      - "8000:8000"
      - "9443:9443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    networks:
      - no-internet
      - custom-network
    restart: unless-stopped
